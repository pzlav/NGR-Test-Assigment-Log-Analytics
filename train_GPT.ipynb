{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "from gpt_model import Transformer, ModelConfig\n",
    "from utils import generate, evaluate, create_datasets, InfiniteDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN params\n",
    "VOCAB_SIZE = 512\n",
    "MAX_LOG_EVENT_LENGTH = 50\n",
    "TOKENS_IN_BLOCK_SIZE = 50\n",
    "EMBEDDING_SIZE = 256\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-ASCII characters from a file\n",
    "def remove_non_ascii(input_file_path, output_file_path):\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        count = 0\n",
    "        for line in infile:\n",
    "            # Remove non-ASCII characters from each line\n",
    "            cleaned_line = ''.join(char for char in line if ord(char) < 128)\n",
    "            outfile.write(cleaned_line)\n",
    "            count += 1\n",
    "            if count == 2_000_000:\n",
    "                break\n",
    "\n",
    "# Example usage\n",
    "input_file_path = 'datasets\\\\applogcat.log'\n",
    "output_file_path = 'datasets\\\\applogcat_clean.log'\n",
    "#remove_non_ascii(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset file\n",
    "file_path = \"datasets/applogcat_clean.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [109, 446, 496, 476, 493, 433, 434, 8, 4, 433, 92, 445, 92, 445, 176, 445, 451, 22, 31, 30, 26, 18, 23, 24, 454, 159, 478, 161, 478, 158, 54, 492, 476, 444, 480, 493]\n",
      "Encoded tokens: ['▁[', 'C', 'L', 'S', ']', '1', '2', '-19', '▁1', '1', ':5', '9', ':5', '9', '.2', '9', '6', '▁3262', '▁3381', '▁I', '▁QCNEJ', '▁:', '▁|', 'CORE', '|', '▁NOTIFY', '_', 'FEATURE', '_', 'STATUS', '▁received', '[', 'S', 'E', 'P', ']']\n",
      "Decoded text: [CLS]12-19 11:59:59.296 3262 3381 I QCNEJ : |CORE| NOTIFY_FEATURE_STATUS received[SEP]\n"
     ]
    }
   ],
   "source": [
    "spm_training_args = f\"--input={file_path} --model_prefix=sentencepiece_bpe --vocab_size={VOCAB_SIZE} \" \\\n",
    "                    f\"--model_type=bpe --unk_piece=[UNK] --pad_piece=[PAD] --bos_piece=[CLS] --eos_piece=[SEP] \" \\\n",
    "                    f\"--user_defined_symbols=[MASK] --hard_vocab_limit=false\"\n",
    "spm.SentencePieceTrainer.Train(spm_training_args)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"sentencepiece_bpe.model\")\n",
    "example_line = '[CLS]12-19 11:59:59.296  3262  3381 I QCNEJ   : |CORE| NOTIFY_FEATURE_STATUS received[SEP]'\n",
    "encoded_ids = sp.EncodeAsIds(example_line)\n",
    "encoded_tokens = sp.EncodeAsPieces(example_line)\n",
    "decoded_text = sp.DecodeIds(encoded_ids)\n",
    "print(f'Encoded IDs: {encoded_ids}')\n",
    "print(f'Encoded tokens: {encoded_tokens}')\n",
    "print(f'Decoded text: {decoded_text}')\n",
    "tokenizer = sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1945863\n",
      "Test set size: 2000\n",
      "LogEventDataset init 1945863\n",
      "LogEventDataset init 2000\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = create_datasets(file_path, tokenizer, MAX_LOG_EVENT_LENGTH)\n",
    "batch_loader = InfiniteDataLoader(train_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: ModelConfig(block_size=50, vocab_size=512, n_embd=256, n_embd2=256, n_head=4, n_layer=4)\n",
      "number of parameters: 3.30M\n",
      "model #params: 3434496\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "my_config = ModelConfig(block_size = TOKENS_IN_BLOCK_SIZE, \n",
    "                        vocab_size = tokenizer.GetPieceSize())\n",
    "print(f\"model config: {my_config}\")\n",
    "model = Transformer(my_config).to(device)\n",
    "print(f\"model #params: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "test_loss_history = []\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr= 1e-3, betas=(0.9, 0.99), eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 | loss 6.3996 | step time 616.57ms\n",
      "step 100 | loss 2.1888 | step time 548.39ms\n",
      "step 200 | loss 1.6799 | step time 556.27ms\n",
      "step 300 | loss 1.3382 | step time 543.41ms\n",
      "step 400 | loss 1.4693 | step time 556.31ms\n",
      "step 500 | loss 1.3059 | step time 542.48ms\n",
      "step 500 train loss: 1.3014250993728638 test loss: 1.2810940742492676\n",
      "test loss 1.2810940742492676 is the best so far\n",
      "step 600 | loss 0.9587 | step time 555.83ms\n",
      "step 700 | loss 1.2161 | step time 581.46ms\n",
      "step 800 | loss 1.3576 | step time 603.27ms\n",
      "step 900 | loss 1.0837 | step time 532.28ms\n",
      "step 1000 | loss 1.0855 | step time 627.80ms\n",
      "step 1000 train loss: 1.1220816373825073 test loss: 1.115871787071228\n",
      "test loss 1.115871787071228 is the best so far\n",
      "step 1100 | loss 0.9005 | step time 581.10ms\n",
      "step 1200 | loss 1.0183 | step time 532.46ms\n",
      "step 1300 | loss 1.1493 | step time 542.39ms\n",
      "step 1400 | loss 1.3055 | step time 519.83ms\n",
      "step 1500 | loss 0.8425 | step time 547.51ms\n",
      "step 1500 train loss: 1.0536514520645142 test loss: 1.0185140371322632\n",
      "test loss 1.0185140371322632 is the best so far\n",
      "step 1600 | loss 0.9557 | step time 583.58ms\n",
      "step 1700 | loss 0.8278 | step time 536.83ms\n",
      "step 1800 | loss 0.9769 | step time 561.48ms\n",
      "step 1900 | loss 0.9772 | step time 603.61ms\n",
      "step 2000 | loss 1.2649 | step time 538.32ms\n",
      "step 2000 train loss: 0.9713059067726135 test loss: 0.9860454201698303\n",
      "test loss 0.9860454201698303 is the best so far\n",
      "step 2100 | loss 1.0149 | step time 529.49ms\n",
      "step 2200 | loss 0.9094 | step time 534.04ms\n",
      "step 2300 | loss 0.8490 | step time 512.99ms\n",
      "step 2400 | loss 0.8749 | step time 587.65ms\n",
      "step 2500 | loss 1.1572 | step time 519.43ms\n",
      "step 2500 train loss: 0.9602920413017273 test loss: 0.9381856918334961\n",
      "test loss 0.9381856918334961 is the best so far\n",
      "step 2600 | loss 1.1068 | step time 573.36ms\n",
      "step 2700 | loss 0.7921 | step time 536.09ms\n",
      "step 2800 | loss 1.0256 | step time 584.28ms\n",
      "step 2900 | loss 0.8707 | step time 582.26ms\n",
      "step 3000 | loss 0.9892 | step time 642.25ms\n",
      "step 3000 train loss: 0.9210793972015381 test loss: 0.9152243137359619\n",
      "test loss 0.9152243137359619 is the best so far\n",
      "step 3100 | loss 0.7852 | step time 568.48ms\n",
      "step 3200 | loss 0.8835 | step time 545.49ms\n",
      "step 3300 | loss 0.9465 | step time 542.55ms\n",
      "step 3400 | loss 0.8690 | step time 610.66ms\n",
      "step 3500 | loss 0.8260 | step time 567.90ms\n",
      "step 3500 train loss: 0.8957002758979797 test loss: 0.8965429067611694\n",
      "test loss 0.8965429067611694 is the best so far\n",
      "step 3600 | loss 0.8383 | step time 548.38ms\n",
      "step 3700 | loss 1.1113 | step time 515.52ms\n",
      "step 3800 | loss 0.8649 | step time 526.33ms\n",
      "step 3900 | loss 0.9824 | step time 546.86ms\n",
      "step 4000 | loss 0.8020 | step time 516.45ms\n",
      "step 4000 train loss: 0.8638712167739868 test loss: 0.8812913298606873\n",
      "test loss 0.8812913298606873 is the best so far\n",
      "step 4100 | loss 0.8338 | step time 538.78ms\n",
      "step 4200 | loss 0.8847 | step time 521.09ms\n",
      "step 4300 | loss 0.9273 | step time 527.01ms\n",
      "step 4400 | loss 1.0308 | step time 506.24ms\n",
      "step 4500 | loss 0.8358 | step time 525.87ms\n",
      "step 4500 train loss: 0.873915433883667 test loss: 0.8787389993667603\n",
      "test loss 0.8787389993667603 is the best so far\n",
      "step 4600 | loss 0.8296 | step time 545.99ms\n",
      "step 4700 | loss 0.7545 | step time 489.92ms\n",
      "step 4800 | loss 0.8861 | step time 559.36ms\n",
      "step 4900 | loss 0.7891 | step time 631.80ms\n",
      "step 5000 | loss 0.9142 | step time 440.69ms\n",
      "step 5000 train loss: 0.8826053738594055 test loss: 0.8548011779785156\n",
      "test loss 0.8548011779785156 is the best so far\n",
      "step 5100 | loss 0.8813 | step time 479.50ms\n",
      "step 5200 | loss 0.8079 | step time 464.20ms\n",
      "step 5300 | loss 0.8309 | step time 445.86ms\n",
      "step 5400 | loss 1.0001 | step time 459.44ms\n",
      "step 5500 | loss 0.7757 | step time 612.09ms\n",
      "step 5500 train loss: 0.8725393414497375 test loss: 0.8772527575492859\n",
      "step 5600 | loss 0.8896 | step time 411.97ms\n",
      "step 5700 | loss 0.7731 | step time 492.23ms\n",
      "step 5800 | loss 0.8205 | step time 500.37ms\n",
      "step 5900 | loss 0.9765 | step time 489.22ms\n",
      "step 6000 | loss 0.7540 | step time 433.80ms\n",
      "step 6000 train loss: 0.8834314346313477 test loss: 0.8354213833808899\n",
      "test loss 0.8354213833808899 is the best so far\n",
      "step 6100 | loss 0.8896 | step time 499.35ms\n",
      "step 6200 | loss 0.8438 | step time 440.56ms\n",
      "step 6300 | loss 0.6866 | step time 439.39ms\n",
      "step 6400 | loss 1.0419 | step time 628.96ms\n",
      "step 6500 | loss 0.7943 | step time 555.05ms\n",
      "step 6500 train loss: 0.8248690962791443 test loss: 0.8653448224067688\n",
      "step 6600 | loss 0.9740 | step time 855.14ms\n",
      "step 6700 | loss 0.8653 | step time 598.51ms\n",
      "step 6800 | loss 0.8137 | step time 599.08ms\n",
      "step 6900 | loss 0.7704 | step time 696.83ms\n",
      "step 7000 | loss 0.9256 | step time 640.33ms\n",
      "step 7000 train loss: 0.8241629004478455 test loss: 0.8530451059341431\n",
      "step 7100 | loss 0.6867 | step time 652.53ms\n",
      "step 7200 | loss 0.7572 | step time 619.33ms\n",
      "step 7300 | loss 0.7475 | step time 644.97ms\n",
      "step 7400 | loss 0.7814 | step time 636.07ms\n",
      "step 7500 | loss 0.9689 | step time 649.56ms\n",
      "step 7500 train loss: 0.8318696618080139 test loss: 0.8328878283500671\n",
      "test loss 0.8328878283500671 is the best so far\n",
      "step 7600 | loss 0.8842 | step time 657.20ms\n",
      "step 7700 | loss 0.7932 | step time 645.31ms\n",
      "step 7800 | loss 0.6931 | step time 623.83ms\n",
      "step 7900 | loss 0.9890 | step time 643.59ms\n",
      "step 8000 | loss 0.7276 | step time 614.17ms\n",
      "step 8000 train loss: 0.8060786724090576 test loss: 0.8367342352867126\n",
      "step 8100 | loss 0.8511 | step time 634.29ms\n",
      "step 8200 | loss 0.9149 | step time 620.05ms\n",
      "step 8300 | loss 0.9144 | step time 633.82ms\n",
      "step 8400 | loss 0.7534 | step time 605.71ms\n",
      "step 8500 | loss 0.9498 | step time 622.10ms\n",
      "step 8500 train loss: 0.8006777167320251 test loss: 0.8413347005844116\n",
      "step 8600 | loss 0.9022 | step time 636.80ms\n",
      "step 8700 | loss 0.9485 | step time 664.91ms\n",
      "step 8800 | loss 0.7082 | step time 641.25ms\n",
      "step 8900 | loss 0.8059 | step time 629.71ms\n",
      "step 9000 | loss 0.7837 | step time 637.52ms\n",
      "step 9000 train loss: 0.8098987340927124 test loss: 0.8209670782089233\n",
      "test loss 0.8209670782089233 is the best so far\n",
      "step 9100 | loss 0.9524 | step time 664.35ms\n",
      "step 9200 | loss 0.7453 | step time 630.74ms\n",
      "step 9300 | loss 0.7676 | step time 648.14ms\n",
      "step 9400 | loss 0.7948 | step time 706.24ms\n",
      "step 9500 | loss 0.7105 | step time 674.46ms\n",
      "step 9500 train loss: 0.8071518540382385 test loss: 0.8168935179710388\n",
      "test loss 0.8168935179710388 is the best so far\n",
      "step 9600 | loss 0.8041 | step time 673.88ms\n",
      "step 9700 | loss 1.0290 | step time 643.70ms\n",
      "step 9800 | loss 0.8719 | step time 655.18ms\n",
      "step 9900 | loss 0.6702 | step time 641.88ms\n",
      "step 10000 | loss 0.8133 | step time 637.02ms\n",
      "step 10000 train loss: 0.7977095246315002 test loss: 0.8096194863319397\n",
      "test loss 0.8096194863319397 is the best so far\n",
      "step 10100 | loss 0.7505 | step time 642.79ms\n",
      "step 10200 | loss 0.7883 | step time 633.19ms\n",
      "step 10300 | loss 0.8429 | step time 600.25ms\n",
      "step 10400 | loss 1.0646 | step time 598.37ms\n",
      "step 10500 | loss 0.9181 | step time 607.12ms\n",
      "step 10500 train loss: 0.7921171188354492 test loss: 0.779587984085083\n",
      "test loss 0.779587984085083 is the best so far\n",
      "step 10600 | loss 0.7764 | step time 604.00ms\n",
      "step 10700 | loss 0.7259 | step time 613.11ms\n",
      "step 10800 | loss 0.9678 | step time 615.76ms\n",
      "step 10900 | loss 0.7713 | step time 645.49ms\n",
      "step 11000 | loss 0.7652 | step time 633.69ms\n",
      "step 11000 train loss: 0.8325716257095337 test loss: 0.7842574715614319\n",
      "step 11100 | loss 0.7371 | step time 626.16ms\n",
      "step 11200 | loss 0.7643 | step time 646.55ms\n",
      "step 11300 | loss 0.7313 | step time 653.13ms\n",
      "step 11400 | loss 0.7071 | step time 702.38ms\n",
      "step 11500 | loss 0.7088 | step time 517.37ms\n",
      "step 11500 train loss: 0.7737898230552673 test loss: 0.8031463623046875\n",
      "step 11600 | loss 0.9030 | step time 457.14ms\n",
      "step 11700 | loss 0.7939 | step time 457.57ms\n",
      "step 11800 | loss 0.7933 | step time 430.82ms\n",
      "step 11900 | loss 0.7792 | step time 444.02ms\n",
      "step 12000 | loss 0.7186 | step time 453.63ms\n",
      "step 12000 train loss: 0.7949125170707703 test loss: 0.7815728783607483\n",
      "step 12100 | loss 0.8516 | step time 476.68ms\n",
      "step 12200 | loss 0.8105 | step time 475.16ms\n",
      "step 12300 | loss 0.7410 | step time 475.81ms\n",
      "step 12400 | loss 0.8699 | step time 495.66ms\n",
      "step 12500 | loss 0.7449 | step time 485.77ms\n",
      "step 12500 train loss: 0.8087471127510071 test loss: 0.7823532223701477\n",
      "step 12600 | loss 0.6552 | step time 535.02ms\n",
      "step 12700 | loss 0.7808 | step time 473.87ms\n",
      "step 12800 | loss 0.8049 | step time 533.19ms\n",
      "step 12900 | loss 0.7634 | step time 504.37ms\n",
      "step 13000 | loss 0.8279 | step time 531.03ms\n",
      "step 13000 train loss: 0.7783228158950806 test loss: 0.7967895269393921\n",
      "step 13100 | loss 0.8885 | step time 509.19ms\n",
      "step 13200 | loss 0.8031 | step time 484.18ms\n",
      "step 13300 | loss 0.7087 | step time 464.60ms\n",
      "step 13400 | loss 0.8121 | step time 440.29ms\n",
      "step 13500 | loss 0.7951 | step time 461.52ms\n",
      "step 13500 train loss: 0.7834833264350891 test loss: 0.7878655791282654\n",
      "step 13600 | loss 0.7247 | step time 423.73ms\n",
      "step 13700 | loss 0.7740 | step time 391.96ms\n",
      "step 13800 | loss 0.7180 | step time 403.52ms\n",
      "step 13900 | loss 0.7734 | step time 408.14ms\n",
      "step 14000 | loss 0.8324 | step time 477.83ms\n",
      "step 14000 train loss: 0.7817539572715759 test loss: 0.7845752239227295\n",
      "step 14100 | loss 0.8748 | step time 502.31ms\n",
      "step 14200 | loss 0.7661 | step time 439.42ms\n",
      "step 14300 | loss 0.8460 | step time 409.56ms\n",
      "step 14400 | loss 0.7196 | step time 471.18ms\n",
      "step 14500 | loss 0.9134 | step time 465.98ms\n",
      "step 14500 train loss: 0.7597326040267944 test loss: 0.7801353335380554\n",
      "step 14600 | loss 0.7095 | step time 436.09ms\n",
      "step 14700 | loss 0.8449 | step time 385.15ms\n",
      "step 14800 | loss 0.7904 | step time 442.13ms\n",
      "step 14900 | loss 0.7040 | step time 801.54ms\n",
      "step 15000 | loss 0.7946 | step time 521.39ms\n",
      "step 15000 train loss: 0.8040181994438171 test loss: 0.7900027632713318\n",
      "step 15100 | loss 0.7358 | step time 491.09ms\n",
      "step 15200 | loss 0.6911 | step time 509.60ms\n",
      "step 15300 | loss 0.7339 | step time 512.00ms\n",
      "step 15400 | loss 0.8728 | step time 495.44ms\n",
      "step 15500 | loss 0.7497 | step time 529.65ms\n",
      "step 15500 train loss: 0.799313485622406 test loss: 0.7944414615631104\n",
      "step 15600 | loss 0.7019 | step time 529.38ms\n",
      "step 15700 | loss 0.7585 | step time 512.71ms\n",
      "step 15800 | loss 0.7270 | step time 532.29ms\n",
      "step 15900 | loss 0.7790 | step time 520.31ms\n",
      "step 16000 | loss 0.8559 | step time 536.90ms\n",
      "step 16000 train loss: 0.7682815194129944 test loss: 0.7905882000923157\n",
      "step 16100 | loss 0.8481 | step time 556.41ms\n",
      "step 16200 | loss 0.6847 | step time 508.20ms\n",
      "step 16300 | loss 0.7197 | step time 542.98ms\n",
      "step 16400 | loss 0.8395 | step time 546.54ms\n",
      "step 16500 | loss 0.8798 | step time 555.41ms\n",
      "step 16500 train loss: 0.7723348140716553 test loss: 0.786958634853363\n",
      "step 16600 | loss 0.7338 | step time 527.27ms\n",
      "step 16700 | loss 0.7296 | step time 508.29ms\n",
      "step 16800 | loss 0.6886 | step time 517.10ms\n",
      "step 16900 | loss 0.7441 | step time 504.95ms\n",
      "step 17000 | loss 0.7635 | step time 497.38ms\n",
      "step 17000 train loss: 0.7633643746376038 test loss: 0.7687061429023743\n",
      "test loss 0.7687061429023743 is the best so far\n",
      "step 17100 | loss 0.7612 | step time 548.68ms\n",
      "step 17200 | loss 0.7323 | step time 528.36ms\n",
      "step 17300 | loss 0.7747 | step time 530.81ms\n",
      "step 17400 | loss 0.9169 | step time 535.57ms\n",
      "step 17500 | loss 0.7627 | step time 520.38ms\n",
      "step 17500 train loss: 0.7948753833770752 test loss: 0.7623643279075623\n",
      "test loss 0.7623643279075623 is the best so far\n",
      "step 17600 | loss 0.7857 | step time 500.45ms\n",
      "step 17700 | loss 0.7099 | step time 521.04ms\n",
      "step 17800 | loss 0.8611 | step time 538.67ms\n",
      "step 17900 | loss 0.7645 | step time 526.34ms\n",
      "step 18000 | loss 0.8627 | step time 525.95ms\n",
      "step 18000 train loss: 0.7506086230278015 test loss: 0.7769773006439209\n",
      "step 18100 | loss 0.7053 | step time 507.59ms\n",
      "step 18200 | loss 0.6925 | step time 504.40ms\n",
      "step 18300 | loss 0.7612 | step time 495.51ms\n",
      "step 18400 | loss 0.7476 | step time 484.04ms\n",
      "step 18500 | loss 0.6786 | step time 504.91ms\n",
      "step 18500 train loss: 0.7610738277435303 test loss: 0.7736303806304932\n",
      "step 18600 | loss 0.8003 | step time 500.08ms\n",
      "step 18700 | loss 0.7666 | step time 506.75ms\n",
      "step 18800 | loss 0.8584 | step time 516.69ms\n",
      "step 18900 | loss 0.7580 | step time 483.10ms\n",
      "step 19000 | loss 0.8121 | step time 506.83ms\n",
      "step 19000 train loss: 0.748744010925293 test loss: 0.7631216645240784\n",
      "step 19100 | loss 0.8322 | step time 492.93ms\n",
      "step 19200 | loss 0.7810 | step time 697.14ms\n",
      "step 19300 | loss 0.7982 | step time 720.75ms\n",
      "step 19400 | loss 0.7350 | step time 733.74ms\n",
      "step 19500 | loss 0.7282 | step time 708.06ms\n",
      "step 19500 train loss: 0.7499677538871765 test loss: 0.7734053134918213\n",
      "step 19600 | loss 0.7628 | step time 727.70ms\n",
      "step 19700 | loss 0.8430 | step time 708.01ms\n",
      "step 19800 | loss 0.7820 | step time 718.93ms\n",
      "step 19900 | loss 0.7383 | step time 712.95ms\n",
      "step 20000 | loss 0.7100 | step time 715.07ms\n",
      "step 20000 train loss: 0.7483779788017273 test loss: 0.7705366015434265\n",
      "step 20100 | loss 0.7502 | step time 1241.57ms\n",
      "step 20200 | loss 0.7963 | step time 892.21ms\n",
      "step 20300 | loss 0.7640 | step time 908.15ms\n",
      "step 20400 | loss 0.8164 | step time 962.69ms\n",
      "step 20500 | loss 0.7561 | step time 1039.97ms\n",
      "step 20500 train loss: 0.7586368918418884 test loss: 0.748126208782196\n",
      "test loss 0.748126208782196 is the best so far\n",
      "step 20600 | loss 0.6857 | step time 951.73ms\n",
      "step 20700 | loss 0.7781 | step time 839.36ms\n",
      "step 20800 | loss 0.7736 | step time 749.75ms\n",
      "step 20900 | loss 0.7593 | step time 685.04ms\n",
      "step 21000 | loss 0.7929 | step time 510.10ms\n",
      "step 21000 train loss: 0.7546859383583069 test loss: 0.7707474231719971\n",
      "step 21100 | loss 0.7445 | step time 449.52ms\n",
      "step 21200 | loss 0.7586 | step time 458.05ms\n",
      "step 21300 | loss 0.6585 | step time 693.38ms\n",
      "step 21400 | loss 0.8570 | step time 1384.57ms\n",
      "step 21500 | loss 0.7339 | step time 959.76ms\n",
      "step 21500 train loss: 0.7773199677467346 test loss: 0.7530124187469482\n",
      "step 21600 | loss 0.6899 | step time 586.00ms\n",
      "step 21700 | loss 0.7237 | step time 572.11ms\n",
      "step 21800 | loss 0.7356 | step time 565.31ms\n",
      "step 21900 | loss 0.7475 | step time 582.34ms\n",
      "step 22000 | loss 0.6640 | step time 571.33ms\n",
      "step 22000 train loss: 0.7687199115753174 test loss: 0.7540680170059204\n",
      "step 22100 | loss 0.7390 | step time 587.35ms\n",
      "step 22200 | loss 0.7399 | step time 534.37ms\n",
      "step 22300 | loss 0.7253 | step time 609.72ms\n",
      "step 22400 | loss 0.7959 | step time 554.27ms\n",
      "step 22500 | loss 0.7773 | step time 573.47ms\n",
      "step 22500 train loss: 0.753109335899353 test loss: 0.7574710845947266\n",
      "step 22600 | loss 0.7723 | step time 573.52ms\n",
      "step 22700 | loss 0.7060 | step time 542.71ms\n",
      "step 22800 | loss 0.7359 | step time 553.76ms\n",
      "step 22900 | loss 0.7211 | step time 543.44ms\n",
      "step 23000 | loss 0.7891 | step time 545.77ms\n",
      "step 23000 train loss: 0.7690402865409851 test loss: 0.7543255090713501\n",
      "step 23100 | loss 0.7505 | step time 564.08ms\n",
      "step 23200 | loss 0.7166 | step time 553.43ms\n",
      "step 23300 | loss 0.7859 | step time 572.10ms\n",
      "step 23400 | loss 0.6994 | step time 537.19ms\n",
      "step 23500 | loss 0.7439 | step time 552.49ms\n",
      "step 23500 train loss: 0.7342300415039062 test loss: 0.7494383454322815\n",
      "step 23600 | loss 0.7001 | step time 536.68ms\n",
      "step 23700 | loss 0.7353 | step time 549.79ms\n",
      "step 23800 | loss 0.8025 | step time 556.50ms\n",
      "step 23900 | loss 0.6894 | step time 572.31ms\n",
      "step 24000 | loss 0.7813 | step time 555.32ms\n",
      "step 24000 train loss: 0.7469159960746765 test loss: 0.7401583194732666\n",
      "test loss 0.7401583194732666 is the best so far\n",
      "step 24100 | loss 0.7231 | step time 576.34ms\n",
      "step 24200 | loss 0.7854 | step time 563.09ms\n",
      "step 24300 | loss 0.7263 | step time 571.41ms\n",
      "step 24400 | loss 0.7766 | step time 553.01ms\n",
      "step 24500 | loss 0.7824 | step time 590.51ms\n",
      "step 24500 train loss: 0.7345162034034729 test loss: 0.7436531186103821\n",
      "step 24600 | loss 0.7492 | step time 580.47ms\n",
      "step 24700 | loss 0.8119 | step time 564.04ms\n",
      "step 24800 | loss 0.7065 | step time 564.89ms\n",
      "step 24900 | loss 0.6653 | step time 561.45ms\n",
      "step 25000 | loss 0.8996 | step time 564.02ms\n",
      "step 25000 train loss: 0.7368690371513367 test loss: 0.7502890229225159\n",
      "step 25100 | loss 0.7481 | step time 534.09ms\n",
      "step 25200 | loss 0.7227 | step time 560.94ms\n",
      "step 25300 | loss 0.7210 | step time 571.87ms\n",
      "step 25400 | loss 1.0147 | step time 568.78ms\n",
      "step 25500 | loss 0.6885 | step time 556.17ms\n",
      "step 25500 train loss: 0.7582060098648071 test loss: 0.7498651742935181\n",
      "step 25600 | loss 0.7384 | step time 571.59ms\n",
      "step 25700 | loss 0.7668 | step time 667.06ms\n",
      "step 25800 | loss 0.7302 | step time 440.21ms\n",
      "step 25900 | loss 0.8113 | step time 654.27ms\n",
      "step 26000 | loss 0.7653 | step time 376.66ms\n",
      "step 26000 train loss: 0.7457979321479797 test loss: 0.7545947432518005\n",
      "step 26100 | loss 0.7170 | step time 503.06ms\n",
      "step 26200 | loss 0.6923 | step time 569.72ms\n",
      "step 26300 | loss 0.7464 | step time 466.24ms\n",
      "step 26400 | loss 0.6660 | step time 452.37ms\n",
      "step 26500 | loss 0.7781 | step time 442.05ms\n",
      "step 26500 train loss: 0.7494146823883057 test loss: 0.7358423471450806\n",
      "test loss 0.7358423471450806 is the best so far\n",
      "step 26600 | loss 0.6819 | step time 415.10ms\n",
      "step 26700 | loss 0.6729 | step time 486.66ms\n",
      "step 26800 | loss 0.7146 | step time 393.26ms\n",
      "step 26900 | loss 0.7747 | step time 503.37ms\n",
      "step 27000 | loss 0.7829 | step time 511.96ms\n",
      "step 27000 train loss: 0.7549798488616943 test loss: 0.7484512329101562\n",
      "step 27100 | loss 0.7772 | step time 513.23ms\n",
      "step 27200 | loss 0.6947 | step time 504.22ms\n",
      "step 27300 | loss 0.7736 | step time 511.24ms\n",
      "step 27400 | loss 0.8628 | step time 493.22ms\n",
      "step 27500 | loss 0.7192 | step time 550.17ms\n",
      "step 27500 train loss: 0.7379724383354187 test loss: 0.7390547394752502\n",
      "step 27600 | loss 0.7088 | step time 641.08ms\n",
      "step 27700 | loss 0.6702 | step time 561.55ms\n",
      "step 27800 | loss 0.7001 | step time 604.88ms\n",
      "step 27900 | loss 0.6969 | step time 750.77ms\n",
      "step 28000 | loss 0.7503 | step time 1066.29ms\n",
      "step 28000 train loss: 0.7636095285415649 test loss: 0.7466464638710022\n",
      "step 28100 | loss 0.7081 | step time 670.68ms\n",
      "step 28200 | loss 0.6620 | step time 534.20ms\n",
      "step 28300 | loss 0.7770 | step time 744.92ms\n",
      "step 28400 | loss 0.9118 | step time 700.98ms\n",
      "step 28500 | loss 0.7555 | step time 619.50ms\n",
      "step 28500 train loss: 0.7363024353981018 test loss: 0.7529032230377197\n",
      "step 28600 | loss 0.8265 | step time 640.91ms\n",
      "step 28700 | loss 0.7385 | step time 558.81ms\n",
      "step 28800 | loss 0.7015 | step time 1047.83ms\n",
      "step 28900 | loss 0.7468 | step time 585.72ms\n",
      "step 29000 | loss 0.9352 | step time 571.09ms\n",
      "step 29000 train loss: 0.7357681393623352 test loss: 0.751800537109375\n",
      "step 29100 | loss 0.8195 | step time 605.70ms\n",
      "step 29200 | loss 0.7292 | step time 540.39ms\n",
      "step 29300 | loss 0.7437 | step time 729.53ms\n",
      "step 29400 | loss 0.7526 | step time 826.98ms\n",
      "step 29500 | loss 0.7220 | step time 748.01ms\n",
      "step 29500 train loss: 0.7429620623588562 test loss: 0.7486177086830139\n",
      "step 29600 | loss 0.6787 | step time 678.56ms\n",
      "step 29700 | loss 0.7230 | step time 577.34ms\n",
      "step 29800 | loss 0.7362 | step time 791.55ms\n",
      "step 29900 | loss 0.7416 | step time 607.63ms\n",
      "step 30000 | loss 0.6778 | step time 622.01ms\n",
      "step 30000 train loss: 0.7422584295272827 test loss: 0.74983811378479\n",
      "step 30100 | loss 0.7250 | step time 602.08ms\n",
      "step 30200 | loss 0.7373 | step time 763.44ms\n",
      "step 30300 | loss 0.8212 | step time 746.61ms\n",
      "step 30400 | loss 0.6507 | step time 724.04ms\n",
      "step 30500 | loss 0.7503 | step time 902.16ms\n",
      "step 30500 train loss: 0.7381750345230103 test loss: 0.7467668652534485\n",
      "step 30600 | loss 0.7339 | step time 585.52ms\n",
      "step 30700 | loss 0.7574 | step time 582.94ms\n",
      "step 30800 | loss 0.6956 | step time 577.03ms\n",
      "step 30900 | loss 0.7415 | step time 730.10ms\n",
      "step 31000 | loss 0.8036 | step time 746.02ms\n",
      "step 31000 train loss: 0.7416210174560547 test loss: 0.7491781711578369\n",
      "step 31100 | loss 0.7235 | step time 618.21ms\n",
      "step 31200 | loss 0.6633 | step time 637.81ms\n",
      "step 31300 | loss 0.7020 | step time 668.10ms\n",
      "step 31400 | loss 0.7864 | step time 596.90ms\n",
      "step 31500 | loss 0.8013 | step time 871.43ms\n",
      "step 31500 train loss: 0.7381207942962646 test loss: 0.758951723575592\n",
      "step 31600 | loss 0.6853 | step time 586.05ms\n",
      "step 31700 | loss 0.7719 | step time 609.82ms\n",
      "step 31800 | loss 0.6634 | step time 709.05ms\n",
      "step 31900 | loss 0.8222 | step time 614.39ms\n",
      "step 32000 | loss 0.7132 | step time 579.80ms\n",
      "step 32000 train loss: 0.7385707497596741 test loss: 0.7480745315551758\n",
      "step 32100 | loss 0.7358 | step time 572.00ms\n",
      "step 32200 | loss 0.6897 | step time 644.66ms\n",
      "step 32300 | loss 0.7158 | step time 702.38ms\n",
      "step 32400 | loss 0.7849 | step time 548.10ms\n",
      "step 32500 | loss 0.7343 | step time 523.79ms\n",
      "step 32500 train loss: 0.7366467118263245 test loss: 0.7339401245117188\n",
      "test loss 0.7339401245117188 is the best so far\n",
      "step 32600 | loss 0.7397 | step time 635.44ms\n",
      "step 32700 | loss 0.8570 | step time 553.08ms\n",
      "step 32800 | loss 0.8197 | step time 538.80ms\n",
      "step 32900 | loss 0.8037 | step time 518.57ms\n",
      "step 33000 | loss 0.6548 | step time 488.75ms\n",
      "step 33000 train loss: 0.7316851019859314 test loss: 0.7364966869354248\n",
      "step 33100 | loss 0.7445 | step time 519.51ms\n",
      "step 33200 | loss 0.6878 | step time 499.92ms\n",
      "step 33300 | loss 0.7511 | step time 477.61ms\n",
      "step 33400 | loss 0.7044 | step time 524.51ms\n",
      "step 33500 | loss 0.6743 | step time 515.98ms\n",
      "step 33500 train loss: 0.7294995784759521 test loss: 0.7486962676048279\n",
      "step 33600 | loss 0.7180 | step time 515.06ms\n",
      "step 33700 | loss 0.7706 | step time 518.18ms\n",
      "step 33800 | loss 0.7516 | step time 486.82ms\n",
      "step 33900 | loss 0.7876 | step time 508.10ms\n",
      "step 34000 | loss 0.8239 | step time 515.91ms\n",
      "step 34000 train loss: 0.7425216436386108 test loss: 0.7344576120376587\n",
      "step 34100 | loss 0.6767 | step time 564.30ms\n",
      "step 34200 | loss 0.7212 | step time 584.79ms\n",
      "step 34300 | loss 0.7531 | step time 558.95ms\n",
      "step 34400 | loss 0.7996 | step time 616.67ms\n",
      "step 34500 | loss 0.6740 | step time 523.61ms\n",
      "step 34500 train loss: 0.7249424457550049 test loss: 0.7484177947044373\n",
      "step 34600 | loss 0.7528 | step time 535.36ms\n",
      "step 34700 | loss 0.7528 | step time 544.40ms\n",
      "step 34800 | loss 0.6673 | step time 557.15ms\n",
      "step 34900 | loss 0.7299 | step time 528.79ms\n",
      "step 35000 | loss 0.6833 | step time 516.17ms\n",
      "step 35000 train loss: 0.7277977466583252 test loss: 0.7509216070175171\n",
      "step 35100 | loss 0.7063 | step time 522.45ms\n",
      "step 35200 | loss 0.6489 | step time 526.32ms\n",
      "step 35300 | loss 0.6659 | step time 646.85ms\n",
      "step 35400 | loss 0.7537 | step time 535.96ms\n",
      "step 35500 | loss 0.7290 | step time 548.79ms\n",
      "step 35500 train loss: 0.7343189716339111 test loss: 0.7345783710479736\n",
      "step 35600 | loss 0.7353 | step time 681.35ms\n",
      "step 35700 | loss 0.7874 | step time 547.56ms\n",
      "step 35800 | loss 0.6735 | step time 995.81ms\n",
      "step 35900 | loss 0.7472 | step time 529.59ms\n",
      "step 36000 | loss 0.7612 | step time 542.76ms\n",
      "step 36000 train loss: 0.7405089735984802 test loss: 0.7380329370498657\n",
      "step 36100 | loss 0.7111 | step time 528.01ms\n",
      "step 36200 | loss 0.6912 | step time 516.85ms\n",
      "step 36300 | loss 0.6942 | step time 525.46ms\n",
      "step 36400 | loss 0.8425 | step time 516.57ms\n",
      "step 36500 | loss 0.7735 | step time 537.88ms\n",
      "step 36500 train loss: 0.7322981357574463 test loss: 0.7302179932594299\n",
      "test loss 0.7302179932594299 is the best so far\n",
      "step 36600 | loss 0.6962 | step time 530.63ms\n",
      "step 36700 | loss 0.7514 | step time 494.78ms\n",
      "step 36800 | loss 0.7149 | step time 540.21ms\n",
      "step 36900 | loss 0.7116 | step time 518.35ms\n",
      "step 37000 | loss 0.6928 | step time 546.40ms\n",
      "step 37000 train loss: 0.7382917404174805 test loss: 0.7477197647094727\n",
      "step 37100 | loss 0.6077 | step time 530.87ms\n",
      "step 37200 | loss 0.6859 | step time 506.16ms\n",
      "step 37300 | loss 0.6380 | step time 616.00ms\n",
      "step 37400 | loss 0.7398 | step time 526.91ms\n",
      "step 37500 | loss 0.8096 | step time 640.49ms\n",
      "step 37500 train loss: 0.7233800292015076 test loss: 0.7446042895317078\n",
      "step 37600 | loss 0.6570 | step time 503.33ms\n",
      "step 37700 | loss 0.7547 | step time 1248.16ms\n",
      "step 37800 | loss 0.7808 | step time 488.46ms\n",
      "step 37900 | loss 0.7883 | step time 515.93ms\n",
      "step 38000 | loss 0.6625 | step time 640.54ms\n",
      "step 38000 train loss: 0.7227990627288818 test loss: 0.7359033823013306\n",
      "step 38100 | loss 0.7066 | step time 575.33ms\n",
      "step 38200 | loss 0.7069 | step time 633.81ms\n",
      "step 38300 | loss 0.6829 | step time 659.15ms\n",
      "step 38400 | loss 0.7597 | step time 604.70ms\n",
      "step 38500 | loss 0.6138 | step time 675.08ms\n",
      "step 38500 train loss: 0.7241109013557434 test loss: 0.7220363020896912\n",
      "test loss 0.7220363020896912 is the best so far\n",
      "step 38600 | loss 0.8149 | step time 904.63ms\n",
      "step 38700 | loss 0.8281 | step time 581.56ms\n",
      "step 38800 | loss 0.7181 | step time 619.95ms\n",
      "step 38900 | loss 0.7176 | step time 718.60ms\n",
      "step 39000 | loss 0.7075 | step time 621.69ms\n",
      "step 39000 train loss: 0.7277953624725342 test loss: 0.7383020520210266\n",
      "step 39100 | loss 0.7147 | step time 836.87ms\n",
      "step 39200 | loss 0.7289 | step time 548.36ms\n",
      "step 39300 | loss 0.7511 | step time 743.97ms\n",
      "step 39400 | loss 0.7222 | step time 741.28ms\n",
      "step 39500 | loss 0.7136 | step time 568.59ms\n",
      "step 39500 train loss: 0.7417272925376892 test loss: 0.7446403503417969\n",
      "step 39600 | loss 0.6645 | step time 698.81ms\n",
      "step 39700 | loss 0.7522 | step time 560.63ms\n",
      "step 39800 | loss 0.7159 | step time 581.62ms\n",
      "step 39900 | loss 0.7277 | step time 556.21ms\n",
      "step 40000 | loss 0.6875 | step time 567.03ms\n",
      "step 40000 train loss: 0.7423414587974548 test loss: 0.7354802489280701\n",
      "step 40100 | loss 0.6638 | step time 939.97ms\n",
      "step 40200 | loss 0.7815 | step time 905.27ms\n",
      "step 40300 | loss 0.7718 | step time 884.13ms\n",
      "step 40400 | loss 0.7215 | step time 1122.80ms\n",
      "step 40500 | loss 0.7200 | step time 1185.35ms\n",
      "step 40500 train loss: 0.7222007513046265 test loss: 0.7289692163467407\n",
      "step 40600 | loss 0.8259 | step time 816.14ms\n",
      "step 40700 | loss 0.6901 | step time 1468.40ms\n",
      "step 40800 | loss 0.7525 | step time 1078.61ms\n",
      "step 40900 | loss 0.6916 | step time 1243.49ms\n",
      "step 41000 | loss 0.6519 | step time 914.50ms\n",
      "step 41000 train loss: 0.7260614037513733 test loss: 0.7256988883018494\n",
      "step 41100 | loss 0.7626 | step time 1126.73ms\n",
      "step 41200 | loss 0.7061 | step time 952.70ms\n",
      "step 41300 | loss 0.7912 | step time 999.57ms\n",
      "step 41400 | loss 0.7061 | step time 721.52ms\n",
      "step 41500 | loss 0.8093 | step time 737.61ms\n",
      "step 41500 train loss: 0.7285327315330505 test loss: 0.7354076504707336\n",
      "step 41600 | loss 0.7600 | step time 820.71ms\n",
      "step 41700 | loss 0.6627 | step time 785.16ms\n",
      "step 41800 | loss 0.7385 | step time 732.84ms\n",
      "step 41900 | loss 0.8692 | step time 742.05ms\n",
      "step 42000 | loss 0.6811 | step time 746.21ms\n",
      "step 42000 train loss: 0.7222092747688293 test loss: 0.7306816577911377\n",
      "step 42100 | loss 0.7091 | step time 766.64ms\n",
      "step 42200 | loss 0.6660 | step time 770.52ms\n",
      "step 42300 | loss 0.7766 | step time 745.48ms\n",
      "step 42400 | loss 0.6870 | step time 733.15ms\n",
      "step 42500 | loss 0.7700 | step time 748.50ms\n",
      "step 42500 train loss: 0.7179326415061951 test loss: 0.7234826683998108\n",
      "step 42600 | loss 0.8184 | step time 802.62ms\n",
      "step 42700 | loss 0.7432 | step time 838.55ms\n",
      "step 42800 | loss 0.7432 | step time 835.89ms\n",
      "step 42900 | loss 0.7474 | step time 819.58ms\n",
      "step 43000 | loss 0.7457 | step time 742.98ms\n",
      "step 43000 train loss: 0.7226489782333374 test loss: 0.727233350276947\n",
      "step 43100 | loss 0.7425 | step time 772.50ms\n",
      "step 43200 | loss 0.7152 | step time 739.35ms\n",
      "step 43300 | loss 0.6775 | step time 753.22ms\n",
      "step 43400 | loss 0.6907 | step time 748.92ms\n",
      "step 43500 | loss 0.7414 | step time 906.06ms\n",
      "step 43500 train loss: 0.7298476696014404 test loss: 0.7392744421958923\n",
      "step 43600 | loss 0.6541 | step time 795.44ms\n",
      "step 43700 | loss 0.8115 | step time 717.60ms\n",
      "step 43800 | loss 0.7190 | step time 733.27ms\n",
      "step 43900 | loss 0.6889 | step time 965.65ms\n",
      "step 44000 | loss 0.7287 | step time 807.50ms\n",
      "step 44000 train loss: 0.7099139094352722 test loss: 0.722661554813385\n",
      "step 44100 | loss 0.8150 | step time 731.26ms\n",
      "step 44200 | loss 0.7305 | step time 783.17ms\n",
      "step 44300 | loss 0.7870 | step time 732.73ms\n",
      "step 44400 | loss 0.7249 | step time 892.44ms\n",
      "step 44500 | loss 0.7578 | step time 732.48ms\n",
      "step 44500 train loss: 0.7250800132751465 test loss: 0.7145488262176514\n",
      "test loss 0.7145488262176514 is the best so far\n",
      "step 44600 | loss 0.7339 | step time 797.68ms\n",
      "step 44700 | loss 0.6808 | step time 753.85ms\n",
      "step 44800 | loss 0.7753 | step time 738.59ms\n",
      "step 44900 | loss 0.7652 | step time 753.26ms\n",
      "step 45000 | loss 0.6346 | step time 733.66ms\n",
      "step 45000 train loss: 0.7266039252281189 test loss: 0.7268168926239014\n",
      "step 45100 | loss 0.7042 | step time 775.32ms\n",
      "step 45200 | loss 0.6546 | step time 803.76ms\n",
      "step 45300 | loss 0.7679 | step time 808.30ms\n",
      "step 45400 | loss 0.6735 | step time 832.14ms\n",
      "step 45500 | loss 0.7011 | step time 851.03ms\n",
      "step 45500 train loss: 0.7215276956558228 test loss: 0.7173928618431091\n",
      "step 45600 | loss 0.6744 | step time 819.93ms\n",
      "step 45700 | loss 0.7384 | step time 774.78ms\n",
      "step 45800 | loss 0.6939 | step time 796.70ms\n",
      "step 45900 | loss 0.7718 | step time 804.07ms\n",
      "step 46000 | loss 0.6791 | step time 744.66ms\n",
      "step 46000 train loss: 0.7116144299507141 test loss: 0.7281836867332458\n",
      "step 46100 | loss 0.6686 | step time 739.08ms\n",
      "step 46200 | loss 0.6972 | step time 747.81ms\n",
      "step 46300 | loss 0.6960 | step time 777.40ms\n",
      "step 46400 | loss 0.7430 | step time 813.19ms\n",
      "step 46500 | loss 0.6665 | step time 871.86ms\n",
      "step 46500 train loss: 0.7149466872215271 test loss: 0.721773624420166\n",
      "step 46600 | loss 0.6589 | step time 741.88ms\n",
      "step 46700 | loss 0.7252 | step time 779.47ms\n",
      "step 46800 | loss 0.6983 | step time 1207.15ms\n",
      "step 46900 | loss 0.7831 | step time 1189.81ms\n",
      "step 47000 | loss 0.6616 | step time 974.37ms\n",
      "step 47000 train loss: 0.7275033593177795 test loss: 0.7293505072593689\n",
      "step 47100 | loss 0.7229 | step time 893.03ms\n",
      "step 47200 | loss 0.7163 | step time 843.11ms\n",
      "step 47300 | loss 0.7620 | step time 760.09ms\n",
      "step 47400 | loss 0.6935 | step time 765.09ms\n",
      "step 47500 | loss 0.7771 | step time 767.88ms\n",
      "step 47500 train loss: 0.7104737758636475 test loss: 0.7271454334259033\n",
      "step 47600 | loss 0.7573 | step time 912.31ms\n",
      "step 47700 | loss 0.7306 | step time 791.53ms\n",
      "step 47800 | loss 0.6624 | step time 857.03ms\n",
      "step 47900 | loss 0.7622 | step time 853.47ms\n",
      "step 48000 | loss 0.6581 | step time 822.37ms\n",
      "step 48000 train loss: 0.7251457571983337 test loss: 0.7218139171600342\n",
      "step 48100 | loss 0.7727 | step time 818.27ms\n",
      "step 48200 | loss 0.7609 | step time 842.45ms\n",
      "step 48300 | loss 0.7126 | step time 1120.10ms\n",
      "step 48400 | loss 0.7384 | step time 876.38ms\n",
      "step 48500 | loss 0.7101 | step time 683.49ms\n",
      "step 48500 train loss: 0.7264986634254456 test loss: 0.7335634827613831\n",
      "step 48600 | loss 0.6691 | step time 578.89ms\n",
      "step 48700 | loss 0.7062 | step time 559.05ms\n",
      "step 48800 | loss 0.6536 | step time 578.65ms\n",
      "step 48900 | loss 0.6949 | step time 548.91ms\n",
      "step 49000 | loss 0.7228 | step time 698.40ms\n",
      "step 49000 train loss: 0.7025906443595886 test loss: 0.7317933440208435\n",
      "step 49100 | loss 0.6532 | step time 641.75ms\n",
      "step 49200 | loss 0.7288 | step time 595.08ms\n",
      "step 49300 | loss 0.6756 | step time 551.78ms\n",
      "step 49400 | loss 0.7601 | step time 540.36ms\n",
      "step 49500 | loss 0.6876 | step time 551.54ms\n",
      "step 49500 train loss: 0.7199287414550781 test loss: 0.7281213998794556\n",
      "step 49600 | loss 0.9251 | step time 619.22ms\n",
      "step 49700 | loss 0.6614 | step time 576.78ms\n",
      "step 49800 | loss 0.6053 | step time 558.76ms\n",
      "step 49900 | loss 0.8379 | step time 530.34ms\n",
      "step 49999 train loss: 0.7208401560783386 test loss: 0.7297585010528564\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "best_loss = None\n",
    "NUM_STEPS = 50_000\n",
    "for step in range(NUM_STEPS):\n",
    "    t0 = time.time()\n",
    "    # get the next batch, ship to device, and unpack it to input and target\n",
    "    batch = batch_loader.next()\n",
    "    batch = [t.to(device) for t in batch]\n",
    "    X, Y = batch\n",
    "    logits, loss = model(X, Y)\n",
    "\n",
    "    # calculate the gradient, update the weights\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t1 = time.time()\n",
    "\n",
    "    # logging\n",
    "    if step % 100 == 0:\n",
    "        print(f\"step {step} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")\n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    # evaluate the model\n",
    "    if step > 0 and step % 500 == 0 or step == NUM_STEPS - 1:\n",
    "        train_loss = evaluate(model, device, train_dataset, batch_size=100, max_batches=10)\n",
    "        test_loss  = evaluate(model, device, test_dataset,  batch_size=100, max_batches=10)\n",
    "        test_loss_history.append((step, test_loss))\n",
    "        print(f\"step {step} train loss: {train_loss} test loss: {test_loss}\")\n",
    "        # save the model to disk if it has improved\n",
    "        if best_loss is None or test_loss < best_loss:\n",
    "            print(f\"test loss {test_loss} is the best so far\")\n",
    "            best_loss = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_line = '12-19 1'\n",
    "tokenizer.EncodeAsIds(example_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-19 12:09:34.514 3262 3381 W QCNEJ : |CORE| UNKOWN Unsolicited Event 8: (ST_REString_ize_id=1660)7707\n",
      "12-19 12:03:34.495 3262 3381 I QCNEJ : |CORE| received protobuf msg:OpertupdateCapClient lase. receivedeleteP7\n",
      "12-19 12:08:59.321 3262 3381 W QCNEJ : |CORE| UNKOWN Unsolicited Event 8d in changecess3KSIONelveDData:\n",
      "12-19 13:00:49.611 31521 14922 D HwActivityManagerService: handleANRFilterFIFO,uid = 10110cmd = 19\n",
      "12-19 12:11:33.241 3262 3381 I QCNEJ : |CORE| received protobuf msg:Oper: Setting ...siIME_De7\n",
      "12-19 12:06:27.097 3262 3381 I QCNEJ : |CORE:COM:RCVR| Disconnected from 'cnd' socket bemoniCache peerkify308 for nc\n",
      "12-19 12:08:50.417 3262 3381 I QCNEJ : |CORE:COM:RCVR| Disconnected from 'cnd' socketmIOExceptionmsched size_subId''bc\n",
      "12-19 12:27:41.472 3262 3371 W QCNEJ : |CORE:COM:SNDR| IOException java.io.IOException: socket closedC\n",
      "12-19 12:01:49.766 2605 16893 D PhoneInterfaceManager: [PhoneIntfMgr] getDataEnabled: subId=0 phoneId=07 portontent=false,calling aus\n",
      "12-19 12:26:26.627 597 1971 E libteec : begin217 ⁇ gthemonitor msg error in call_id=0x}\n"
     ]
    }
   ],
   "source": [
    "example_of_start = torch.tensor([1]).reshape(1,-1)\n",
    "example_of_start.to(device)\n",
    "model.to(device) \n",
    "for _ in range(10):\n",
    "    res = \"\"\n",
    "    gen_ids = generate(model, device, example_of_start, max_new_tokens=MAX_LOG_EVENT_LENGTH, do_sample=True, temperature=1, top_k=20).reshape(-1).tolist()\n",
    "    res = tokenizer.DecodeIds(gen_ids)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
